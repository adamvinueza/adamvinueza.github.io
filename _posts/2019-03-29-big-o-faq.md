---
layout: post
title:  "Big-O FAQ"
date:   2019-03-29 11:23:59 -0600
categories: algorithms
---
In my last post, I discussed the reasons for studying big-O algorithm analysis.
This post answers a few questions I often get about big-O.

## Frequently asked questions about big-O

### What does the 'O' stand for?

The 'O' stands for the _order_ of the function. In mathematics, an _ordering_ is
just a sequence in a particular order. Magnitudes of 10, for example, have an
order, based on the exponent to which 10 is raised: the higher the number, the
higher the _order of magnitude_.

The most commonly encountered categories of orders of functions are as follows,
in increasing order of complexity:

- *Constant*: O(_1_) indicates that there is a constant number of operations
  regardless of the input to the function.
- *Logarithmic*: O(_log n_) indicates that the number of operations increases as a
  base-2 logarithmic function of the input to the function: the number of
  operations is no larger than log (base 2) of n.
- *Linear*: O(_n_) indicates that the number of operations increases as a linear
  function of the input to the function. We call it "linear" because if you
  graph the number of operations against the input, the graph is a straight
  line.
- *Quadratic*: O(_n_<sup>_k_</sup>) indicates that the number of operations as a
  quadratic function of the input to the function: the value k is a constant,
  but just isn't made specific here--it could be 2, or 4, or 758. Different
  values of k present different orders within this category.
- *Exponential*: O(_k_<sup>_n_</sup>) indicates that the number of operations
  increases as an exponential function of the input to the function. The higher
  the value of _k_, the higher the order within this category.

There are of course orders in between: for example, O(_n log n_) is between Linear
and Quadratic.

### How hard is the mathematics of determining an algorithm's complexity?

For most of the algorithms you'll encounter, the mathematics required goes no
farther than high-school algebra, but the complexity of the mathematics in
general depends on the algorithm. That said, the overwhelming majority of the
algorithms you're likely to encounter require no more than elementary calculus
to accurately determine their complexity.

### What's an example of the math involved in complexity analysis?

This is the "Can you hum a few bars for me so I can fake it?" question, and it's
a _great_ question. If you want to learn complexity analysis, watch someone do
it!

Linked-list search is a good example of analysis that requires only basic
algebra. Let's define a linked list in Python:
```python
class Node:
    __slots__ = [ 'key', 'next', 'prev' ]
    def __init__(self, x):
        self.key = x
        self.next = None
        self.prev = None

class LinkedList:
    __slots__ = [ 'head' ]

    def __init__(self):
        self.head = None

    def insert(self, x):
        node = Node(x)
        node.next = self.head
        if self.head is not None:
            self.head.prev = node
        self.head = node
        
    def delete(self, node):
        if node is not None:
            if node.prev is not None:
                node.prev.next = node.next
            else:
                self.head = node.next
            if node.next is not None:
                node.next.prev = n.prev

    def search(self, x):
        node = self.head
        while node is not None and node.key != x:
            node = node.next
        return node
```
Take a minute or two to make sure you understand this code at a high level. The
main ideas:
- a linked list contains nodes with pointers to previous and next nodes;
- a linked list has a head node;
- inserting a node always puts the new node at the front.

OK, ready? It's easy to see that inserting into and deleting from into a linked
list is O(_1_): each line in those functions involves at most two operations
(such as a call to access a property of a node, conjoined with a call assigning
its value to another property of a node), and neither contains any kind of loop,
so the number of operations does not grow as the list gets larger.

The search operation is different: finding a node takes longer as the list gets
longer, but only in proportion to the size of the list. To say that it's
proportional is just to say that the number of operations always stays within
some constant factor of the size of the list. The more formal way of putting
that is to say `LinkedList.search` is O(_n_). Let's prove that.

In `LinkedList.search`, there is the call to `head` on the list, the initial
setting of `node` to the result of calling `head`, the return of `node`,
and four operations for each node _e_ of the list:
- check that _e_ is not `None`;
- check that _e.key_ != _x_;
- call _e.next_;
- assign the value of _e.next_ to _e_.

A more formal way of putting this is to let _f_ be the function describing the
running time of `LinkedList.search`, and to define _f_ like this:
```
f(n) = 4n + 3
```
It's easy to prove that _f(n)_ = O(_n_). Recall our definition of O(_n_): there
is a constant _c_ such that _f(n) < cn_. Substituting in the definition of _f_,
we get:
```
4n + 3 < cn
```
Dividing by _n_ gives us the required inequality: `4 + 3/n < c`. Clearly, as
_n_ grows larger, _3/n_ approaches zero, so the real inequality is: `4 < c`.
Many numbers satisfy this inequality; therefore, `LinkedList.search` is O(_n_).

Note that you'll almost never actually have to construct a proof like this. So
why do it at all? Because constructing the proof shows you _how to think_ about
analysing the running time of the algorithm. The point is not to construct
rigorous proofs, but to learn how to think rigorously about running times.

The key aspects of thinking rigorously about running times are illustrated in
the analysis of the Python code. We did two things:
- count the number of operations in the function that didn't depend on the
  number of `Node` objects in the `LinkedList` (3 total);
- count the number of operations that did depend on the number of `Node`
  objects, by multiplying the number of operations in each loop iteration (4
  total) by the number of iterations (i.e., the number of `Node` objects).

The rest of the analysis was just showing that the number we arrived at through
counting in fact satisfied the definition of O(_n_).

You won't find yourself doing this sort of analysis very often, but the
important part is _knowing how_ to do it. Knowing how to do this analysis gives
one a good feel for how algorithms should perform, and allows you to make decent
(and _defensible_!) snap judgments about how a program should behave given how
its algorithms are implemented.

### What does '_n_' stand for in the expression O(_f(n)_)?

The precise meaning of '_n_' in O(_f(n)_) depends on the algorithm itself. If
you say that an algorithm's complexity is O(_f(n)_), it's your responsibility to
specify what 'n' means. We talk broadly about "input", but the input might not
be explicit in the function's signature. For example, consider this Python
function:
```python
def print_squared(numeric_string):
    n = int(numeric_string)
    sq = n*n
    for i in range(sq):
        print(i)
```
The `print_squared` function has a complexity of O(_n_<sup>_2_</sup>), even
though the input to the function, strictly speaking, is a single string.  Here,
the function's complexity depends on the size of the integer value of the
string, not on the number of characters in the string or on the number of
strings. The precise thing to say, then, is that _f(n) = n<sup>2</sup>_, so the
number of operations grows in proportion to the square of the number
_represented_ by the function's input string.

### How can I know the complexity of my favorite language's library functions?

You can often find out a library function's complexity by reading its
documentation, because the documentation specifies the data structures
associated with that function, and those are typically well understood. If the
function is an operation on a well-understood data structure, and you know the
complexity of that particular operation, you thereby know the function's
complexity.

For example, according to Go's documentation, `containers/list` struct instances
are implemented as doubly-linked lists. This implies that insertion and
retrieval from the front of a list are both O(_1_) operations, and that finding an
element of a list is an O(_n_) operation, where 'n' stands for the number of
elements in the list. (In a `containers\list` struct in Go, these are the
`PushFront` and `Front` functions respectively.)

On the other hand, a library's documentation may say little or nothing about a
particular function's complexity. In that case, you may have no recourse other
than reading the library's source code for that function.

### What resources are there for learning about algorithm analysis?

There are many online resources for studying algorithms, but I haven't found
anything better than the ones discussed below, which are all books available for
order at Amazon. You may be able to find older pirated PDF versions online. The
texts are listed in decreasing order of difficulty.

The classic text is [_Introduction to
Algorithms_](https://www.amazon.com/gp/product/B007CNRCAO/ref=dbs_a_def_rwt_hsch_vapi_taft_p1_i0),
by Thomas Cormen, Charles Leiserson, Ronald Rivest, and Clifford Stein.
Comprehensive and well written, but often extremely challenging. It repays close
reading and study. I used this book in both undergraduate and graduate classes
in algorithms, and still refer to it occasionally.

A much more accessible text is Steven Skiena's [_The Algorithm Design
Manual_](http://www.algorist.com). I've read only little bits of it, but it's
readable and accessible. One of the more popular texts in computer science
programs. 

By far the most accessible--but also the least comprehensive--is [_Grokking
Algorithms_](https://www.manning.com/books/grokking-algorithms), by Aditya
Bhargava. It covers many of the fundamental algorithms and data structures, is
clearly and energetically written, and includes many helpful diagrams.  Because
it's meant for a far more general reader than most algorithms textbooks, it
treads very lightly on the mathematics, and has little in the way of programming
exercises. It also has scant coverage of several important data structures, such
as stacks, queues, and trees. But for an introduction to algorithms and data
structures that's gentle without dumbing down the material, it's hard to top.
